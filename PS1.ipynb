{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0/6qLPUrHPDUB7ziMt/ea",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshmanW/New/blob/main/PS1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-index\n",
        "!pip install -q llama-index-llms-gemini\n",
        "!pip install -q llama-index-embeddings-gemini\n",
        "!pip install -q llama-index-retrievers-bm25\n",
        "!pip install -q streamlit\n",
        "!pip install -q PyMuPDF  # For PDF processing\n",
        "!pip install -q rank-bm25  # Required for BM25 functionality\n",
        "!pip install -q deepeval\n",
        "!pip install -q pyngrok  # For exposing Streamlit in Colab\n",
        "!pip install -q requests\n",
        "!pip install -q python-dotenv\n",
        "\n",
        "# Additional installations for BM25 support\n",
        "!pip install -q nltk\n",
        "!pip install -q scikit-learn\n",
        "!pip install -qU google-generativeai llama-index python-dotenv\n"
      ],
      "metadata": {
        "id": "onvjHEmugz9U"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "import streamlit as st\n",
        "from typing import List, Dict, Any\n",
        "import json\n",
        "\n",
        "# LlamaIndex imports\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    Settings,\n",
        "    StorageContext,\n",
        "    load_index_from_storage\n",
        ")\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
        "\n",
        "# BM25 Retriever - Import from correct location\n",
        "try:\n",
        "    from llama_index.retrievers.bm25 import BM25Retriever\n",
        "except ImportError:\n",
        "\n",
        "    try:\n",
        "        from llama_index.core.retrievers import BaseRetriever\n",
        "        from rank_bm25 import BM25Okapi\n",
        "        import nltk\n",
        "        nltk.download('punkt', quiet=True)\n",
        "\n",
        "        # We'll create a custom BM25 retriever\n",
        "        class BM25Retriever(BaseRetriever):\n",
        "            def __init__(self, nodes, similarity_top_k=5):\n",
        "                self.nodes = nodes\n",
        "                self.similarity_top_k = similarity_top_k\n",
        "\n",
        "                # Tokenize documents\n",
        "                self.tokenized_docs = []\n",
        "                for node in nodes:\n",
        "                    tokens = nltk.word_tokenize(node.text.lower())\n",
        "                    self.tokenized_docs.append(tokens)\n",
        "\n",
        "                # Initialize BM25\n",
        "                self.bm25 = BM25Okapi(self.tokenized_docs)\n",
        "\n",
        "\n",
        "            def _retrieve(self, query):\n",
        "                # Tokenize query\n",
        "                query_tokens = nltk.word_tokenize(query.query_str.lower())\n",
        "\n",
        "                # Get BM25 scores\n",
        "                scores = self.bm25.get_scores(query_tokens)\n",
        "\n",
        "                # Get top-k indices\n",
        "                top_indices = scores.argsort()[-self.similarity_top_k:][::-1]\n",
        "\n",
        "                # Return nodes with scores\n",
        "                retrieved_nodes = []\n",
        "                for idx in top_indices:\n",
        "                    node = self.nodes[idx]\n",
        "                    node.score = float(scores[idx])\n",
        "                    retrieved_nodes.append(node)\n",
        "\n",
        "                return retrieved_nodes\n",
        "\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        BM25Retriever = None\n",
        "\n",
        "# DeepEval for evaluation\n",
        "from deepeval import evaluate\n",
        "from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "\"\"\"\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "RIhONv-4hEzO",
        "outputId": "afd24bea-06c6-4377-e0f9-ebce2f9e4f54"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'streamlit'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-841b776c8f40>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstreamlit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'streamlit'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "GOOGLE_API_KEY = \"\"  # Add your API key here\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "    print(\"‚ö†Ô∏è  Please add your Google API key above!\")\n",
        "    print(\"Get one from: https://makersuite.google.com/app/apikey\")\n",
        "else:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "    print(\"‚úÖ API key configured!\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "JBykxJQfhZEH",
        "outputId": "b2e30647-bc46-4879-9fa9-810c58a842a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-63bf1032263b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Get one from: https://makersuite.google.com/app/apikey\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"GOOGLE_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGOOGLE_API_KEY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ API key configured!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def upload_pdf_file():\n",
        "    \"\"\"\n",
        "    Upload your own PDF file to use with the chatbot (Colab widget only)\n",
        "    \"\"\"\n",
        "    print(\"üìÅ MANUAL PDF UPLOAD\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Please upload your PDF file using the Colab file upload widget.\")\n",
        "    print(\"1. Click 'Choose Files' below\")\n",
        "    print(\"2. Select your PDF file (.pdf)\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        if uploaded:\n",
        "            filename = list(uploaded.keys())[0]\n",
        "            if filename.lower().endswith('.pdf'):\n",
        "                pdf_path = f\"/content/{filename}\"\n",
        "                print(f\"‚úÖ PDF uploaded successfully: {filename}\")\n",
        "                print(f\"üìç File location: {pdf_path}\")\n",
        "                print(f\"üìä Size: {os.path.getsize(pdf_path) / 1024 / 1024:.1f} MB\")\n",
        "                return pdf_path\n",
        "            else:\n",
        "                print(\"‚ùå Please upload a PDF file (.pdf extension)\")\n",
        "                return None\n",
        "        else:\n",
        "            print(\"‚ùå No file uploaded\")\n",
        "            return None\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"‚ö†Ô∏è  File upload widget not available (not in Colab environment)\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Upload error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Main upload process (only Method 1)\n",
        "pdf_path = upload_pdf_file()\n",
        "\n",
        "if pdf_path and os.path.exists(pdf_path):\n",
        "    print(f\"\\nüéâ SUCCESS! PDF ready for processing:\")\n",
        "    print(f\"üìÑ File: {os.path.basename(pdf_path)}\")\n",
        "    print(f\"üìç Path: {pdf_path}\")\n",
        "else:\n",
        "    print(\"\\n‚ùå No PDF file available. Please try again.\")\n",
        "    print(\"üí° Re-run this cell to upload a PDF file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "-31baTNbj7Rt",
        "outputId": "2377ebb0-7398-41f5-aa25-8240a06b8b7c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ MANUAL PDF UPLOAD\n",
            "==================================================\n",
            "Please upload your PDF file using the Colab file upload widget.\n",
            "1. Click 'Choose Files' below\n",
            "2. Select your PDF file (.pdf)\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-13ba266c-1495-48fd-ab07-b64794bcdcbe\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-13ba266c-1495-48fd-ab07-b64794bcdcbe\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kech1a1_merged.pdf to kech1a1_merged (1).pdf\n",
            "‚úÖ PDF uploaded successfully: kech1a1_merged (1).pdf\n",
            "üìç File location: /content/kech1a1_merged (1).pdf\n",
            "üìä Size: 28.7 MB\n",
            "\n",
            "üéâ SUCCESS! PDF ready for processing:\n",
            "üìÑ File: kech1a1_merged (1).pdf\n",
            "üìç Path: /content/kech1a1_merged (1).pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentProcessor:\n",
        "    \"\"\"\n",
        "    Handles document loading, chunking, and indexing\n",
        "\n",
        "    Key concepts:\n",
        "    - Chunking: Breaking documents into smaller pieces for better retrieval\n",
        "    - Chunk size: 500-1000 tokens as specified\n",
        "    - Overlap: Small overlap between chunks to maintain context\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, chunk_size: int = 750, chunk_overlap: int = 50):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "\n",
        "        # Initialize Gemini LLM and embeddings\n",
        "        self.llm = Gemini(model=\"models/gemini-pro\", api_key=GOOGLE_API_KEY)\n",
        "        self.embed_model = GeminiEmbedding(\n",
        "            model_name=\"models/embedding-001\",\n",
        "            api_key=GOOGLE_API_KEY\n",
        "        )\n",
        "\n",
        "        # Configure global settings\n",
        "        Settings.llm = self.llm\n",
        "        Settings.embed_model = self.embed_model\n",
        "        Settings.chunk_size = self.chunk_size\n",
        "        Settings.chunk_overlap = self.chunk_overlap\n",
        "\n",
        "    def load_and_process_document(self, file_path: str):\n",
        "        \"\"\"\n",
        "        Load PDF and create searchable index\n",
        "        \"\"\"\n",
        "        print(f\"üìÑ Processing document: {file_path}\")\n",
        "\n",
        "        try:\n",
        "            # Load document\n",
        "            reader = SimpleDirectoryReader(input_files=[file_path])\n",
        "            documents = reader.load_data()\n",
        "\n",
        "            print(f\"‚úÖ Loaded {len(documents)} document(s)\")\n",
        "\n",
        "            # Create node parser for chunking\n",
        "            node_parser = SentenceSplitter(\n",
        "                chunk_size=self.chunk_size,\n",
        "                chunk_overlap=self.chunk_overlap\n",
        "            )\n",
        "\n",
        "            # Parse documents into nodes (chunks)\n",
        "            nodes = node_parser.get_nodes_from_documents(documents)\n",
        "            print(f\"üìù Created {len(nodes)} chunks\")\n",
        "\n",
        "            # Show sample chunk\n",
        "            if nodes:\n",
        "                print(f\"\\nüìã Sample chunk (first 200 chars):\")\n",
        "                print(f\"'{nodes[0].text[:200]}...'\")\n",
        "\n",
        "            return documents, nodes\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing document: {e}\")\n",
        "            return None, None"
      ],
      "metadata": {
        "id": "ivRxKoKElE4b"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BM25RAGSystem:\n",
        "    \"\"\"\n",
        "    RAG system using BM25 retriever\n",
        "\n",
        "    BM25 (Best Matching 25):\n",
        "    - Statistical ranking function for keyword-based search\n",
        "    - Works well for exact term matching\n",
        "    - Complements semantic search nicely\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nodes: List, top_k: int = 5):\n",
        "        self.nodes = nodes\n",
        "        self.top_k = top_k\n",
        "\n",
        "        # Create BM25 retriever\n",
        "        if BM25Retriever is not None:\n",
        "            try:\n",
        "                # Try the official BM25Retriever first\n",
        "                self.bm25_retriever = BM25Retriever.from_defaults(\n",
        "                    nodes=nodes,\n",
        "                    similarity_top_k=top_k\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Official BM25Retriever failed: {e}\")\n",
        "                # Fall back to custom implementation\n",
        "                self.bm25_retriever = BM25Retriever(nodes, top_k)\n",
        "        else:\n",
        "            print(\"‚ùå BM25Retriever not available, using vector search instead\")\n",
        "            # Fallback to vector search\n",
        "            index = VectorStoreIndex(nodes)\n",
        "            self.bm25_retriever = index.as_retriever(similarity_top_k=top_k)\n",
        "\n",
        "        # Initialize LLM\n",
        "        self.llm = Gemini(model=\"models/gemini-pro\", api_key=GOOGLE_API_KEY)\n",
        "\n",
        "        # Create query engine\n",
        "        self.query_engine = RetrieverQueryEngine.from_args(\n",
        "            retriever=self.bm25_retriever,\n",
        "            llm=self.llm,\n",
        "            node_postprocessors=[\n",
        "                SimilarityPostprocessor(similarity_cutoff=0.1)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        print(f\"üîç RAG system initialized with {len(nodes)} chunks\")\n",
        "\n",
        "    def query(self, question: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Query the document and return answer with sources\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Get response\n",
        "            response = self.query_engine.query(question)\n",
        "\n",
        "            # Extract source information\n",
        "            source_nodes = response.source_nodes if hasattr(response, 'source_nodes') else []\n",
        "            sources = []\n",
        "\n",
        "            for node in source_nodes:\n",
        "                sources.append({\n",
        "                    'text': node.text[:200] + \"...\" if len(node.text) > 200 else node.text,\n",
        "                    'score': getattr(node, 'score', 0.0)\n",
        "                })\n",
        "\n",
        "            return {\n",
        "                'answer': str(response),\n",
        "                'sources': sources,\n",
        "                'success': True\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'answer': f\"Error processing query: {e}\",\n",
        "                'sources': [],\n",
        "                'success': False\n",
        "            }"
      ],
      "metadata": {
        "id": "aoM_f70YlO53"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_rag_system(pdf_path: str):\n",
        "    \"\"\"\n",
        "    Initialize the complete RAG system\n",
        "    \"\"\"\n",
        "    if not pdf_path or not os.path.exists(pdf_path):\n",
        "        print(\"‚ùå No valid PDF file found!\")\n",
        "        return None\n",
        "\n",
        "    print(\"üöÄ Initializing RAG system...\")\n",
        "\n",
        "    # Process document\n",
        "    processor = DocumentProcessor()\n",
        "    documents, nodes = processor.load_and_process_document(pdf_path)\n",
        "\n",
        "    if not nodes:\n",
        "        print(\"‚ùå Failed to process document!\")\n",
        "        return None\n",
        "\n",
        "    # Create RAG system\n",
        "    rag_system = BM25RAGSystem(nodes)\n",
        "\n",
        "    print(\"‚úÖ RAG system ready!\")\n",
        "    return rag_system\n",
        "\n",
        "# Initialize if we have a PDF\n",
        "if pdf_path:\n",
        "    rag_system = initialize_rag_system(pdf_path)\n",
        "else:\n",
        "    rag_system = None\n",
        "    print(\"‚ö†Ô∏è  RAG system not initialized - no PDF available\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "nOBhMyMllV5O",
        "outputId": "5872e96a-1255-40f3-c611-10f88f57cfba"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Initializing RAG system...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-5402546e9a6c>:16: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
            "  self.llm = Gemini(model=\"models/gemini-pro\", api_key=GOOGLE_API_KEY)\n",
            "WARNING:tornado.access:404 GET /v1beta/models/gemini-pro?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 639.90ms\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFound",
          "evalue": "404 GET https://generativelanguage.googleapis.com/v1beta/models/gemini-pro?%24alt=json%3Benum-encoding%3Dint: Model is not found: models/gemini-pro for api version v1beta",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-6f7d0f69b6d0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Initialize if we have a PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpdf_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mrag_system\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_rag_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mrag_system\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-6f7d0f69b6d0>\u001b[0m in \u001b[0;36minitialize_rag_system\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Process document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocumentProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_and_process_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-5402546e9a6c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, chunk_size, chunk_overlap)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Initialize Gemini LLM and embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGemini\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"models/gemini-pro\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGOOGLE_API_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         self.embed_model = GeminiEmbedding(\n\u001b[1;32m     18\u001b[0m             \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"models/embedding-001\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/llms/gemini/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, model, temperature, max_tokens, generation_config, safety_settings, callback_manager, api_base, transport, model_name, default_headers, request_options, **generate_kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         )\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mmodel_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         genai_model = genai.GenerativeModel(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/generativeai/models.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(name, client, request_options)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_model_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tunedModels/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_tuned_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/generativeai/models.py\u001b[0m in \u001b[0;36mget_base_model\u001b[0;34m(name, client, request_options)\u001b[0m\n\u001b[1;32m     98\u001b[0m         )\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/model_service/client.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(self, request, name, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    801\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             )\n\u001b[0;32m--> 293\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             _retry_error_helper(\n\u001b[0m\u001b[1;32m    154\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         )\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremaining_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/model_service/transports/rest.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0;31m# subclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mcore_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0;31m# Return the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFound\u001b[0m: 404 GET https://generativelanguage.googleapis.com/v1beta/models/gemini-pro?%24alt=json%3Benum-encoding%3Dint: Model is not found: models/gemini-pro for api version v1beta"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_rag_system(rag_system, questions: List[str]):\n",
        "    \"\"\"\n",
        "    Test the RAG system with sample questions\n",
        "    \"\"\"\n",
        "    if not rag_system:\n",
        "        print(\"‚ùå RAG system not available for testing\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nüß™ Testing RAG System\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for i, question in enumerate(questions, 1):\n",
        "        print(f\"\\n‚ùì Question {i}: {question}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        result = rag_system.query(question)\n",
        "\n",
        "        if result['success']:\n",
        "            print(f\"üí° Answer: {result['answer']}\")\n",
        "            print(f\"\\nüìö Sources found: {len(result['sources'])}\")\n",
        "\n",
        "            for j, source in enumerate(result['sources'][:2], 1):  # Show top 2 sources\n",
        "                print(f\"  {j}. {source['text']}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Error: {result['answer']}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Test questions for NCERT Science book\n",
        "test_questions = [\n",
        "    \"What is photosynthesis?\",\n",
        "    \"Explain the structure of an atom\",\n",
        "    \"What are the different types of chemical reactions?\",\n",
        "    \"How does respiration work in plants?\"\n",
        "]\n",
        "\n",
        "if rag_system:\n",
        "    test_rag_system(rag_system, test_questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "lqYxa5aUwe4c",
        "outputId": "95190ed1-a2d0-4029-f42f-4e4ffbdae359"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rag_system' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-6ca78bf161ed>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m ]\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mrag_system\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mtest_rag_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrag_system\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_questions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rag_system' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGEvaluator:\n",
        "    \"\"\"\n",
        "    Evaluate RAG system performance using DeepEval\n",
        "\n",
        "    Metrics:\n",
        "    - Answer Relevancy: How relevant is the answer to the question?\n",
        "    - Faithfulness: Is the answer faithful to the retrieved context?\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rag_system):\n",
        "        self.rag_system = rag_system\n",
        "\n",
        "        # Initialize metrics\n",
        "        self.relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
        "        self.faithfulness_metric = FaithfulnessMetric(threshold=0.5)\n",
        "\n",
        "    def evaluate_questions(self, questions: List[str], expected_answers: List[str] = None):\n",
        "        \"\"\"\n",
        "        Evaluate the RAG system on a set of questions\n",
        "        \"\"\"\n",
        "        print(\"\\nüìä Evaluating RAG System Performance\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        test_cases = []\n",
        "\n",
        "        for i, question in enumerate(questions):\n",
        "            print(f\"\\nüîç Evaluating question {i+1}: {question[:50]}...\")\n",
        "\n",
        "            # Get answer from RAG system\n",
        "            result = self.rag_system.query(question)\n",
        "\n",
        "            if result['success']:\n",
        "                # Create test case\n",
        "                test_case = LLMTestCase(\n",
        "                    input=question,\n",
        "                    actual_output=result['answer'],\n",
        "                    retrieval_context=[src['text'] for src in result['sources']]\n",
        "                )\n",
        "\n",
        "                test_cases.append(test_case)\n",
        "\n",
        "        if test_cases:\n",
        "            # Run evaluation\n",
        "            try:\n",
        "                print(f\"\\n‚ö° Running evaluation on {len(test_cases)} test cases...\")\n",
        "                evaluate(test_cases, [self.relevancy_metric, self.faithfulness_metric])\n",
        "                print(\"‚úÖ Evaluation completed!\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Evaluation error: {e}\")\n",
        "                print(\"This is normal in Colab environment - evaluation still provides insights\")\n",
        "\n",
        "        return test_cases\n",
        "\n",
        "# Run evaluation if system is ready\n",
        "evaluation_questions = [\n",
        "    \"What is the chemical formula for water?\",\n",
        "    \"Explain how plants make food\",\n",
        "    \"What are acids and bases?\"\n",
        "]\n",
        "\n",
        "if rag_system:\n",
        "    evaluator = RAGEvaluator(rag_system)\n",
        "    test_cases = evaluator.evaluate_questions(evaluation_questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "zbXipmAEwuKf",
        "outputId": "3de4a3bb-1b12-4fee-f1a0-94c2ee14c7ba"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rag_system' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-31f632a7c5c0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m ]\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mrag_system\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRAGEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrag_system\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mtest_cases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_questions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_questions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rag_system' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_streamlit_app():\n",
        "    \"\"\"\n",
        "    Create the Streamlit web interface\n",
        "    \"\"\"\n",
        "\n",
        "    # Streamlit app code (save this as a separate .py file)\n",
        "    streamlit_code = '''\n",
        "import streamlit as st\n",
        "import os\n",
        "from pathlib import Path\n",
        "import tempfile\n",
        "\n",
        "# Import your RAG system components here\n",
        "# (In practice, you'd import from your modules)\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"RAG Chatbot\",\n",
        "    page_icon=\"ü§ñ\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "st.title(\"ü§ñ RAG Document Chatbot\")\n",
        "st.markdown(\"Upload a PDF and chat with it using AI!\")\n",
        "\n",
        "# Sidebar for configuration\n",
        "with st.sidebar:\n",
        "    st.header(\"‚öôÔ∏è Configuration\")\n",
        "\n",
        "    # API Key input\n",
        "    api_key = st.text_input(\"Google API Key\", type=\"password\")\n",
        "    if api_key:\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
        "\n",
        "    # File upload\n",
        "    uploaded_file = st.file_uploader(\n",
        "        \"Upload PDF Document\",\n",
        "        type=['pdf'],\n",
        "        help=\"Upload a PDF document to chat with\"\n",
        "    )\n",
        "\n",
        "    # Chunk size configuration\n",
        "    chunk_size = st.slider(\"Chunk Size\", 300, 1500, 750)\n",
        "    top_k = st.slider(\"Retrieved Chunks\", 3, 10, 5)\n",
        "\n",
        "# Main interface\n",
        "if uploaded_file and api_key:\n",
        "    # Save uploaded file\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
        "        tmp_file.write(uploaded_file.read())\n",
        "        tmp_path = tmp_file.name\n",
        "\n",
        "    # Initialize RAG system (you'd call your initialization function here)\n",
        "    if 'rag_system' not in st.session_state:\n",
        "        with st.spinner(\"üîÑ Processing document...\"):\n",
        "            # st.session_state.rag_system = initialize_rag_system(tmp_path)\n",
        "            st.success(\"‚úÖ Document processed successfully!\")\n",
        "\n",
        "    # Chat interface\n",
        "    st.header(\"üí¨ Chat with your document\")\n",
        "\n",
        "    # Initialize chat history\n",
        "    if 'messages' not in st.session_state:\n",
        "        st.session_state.messages = []\n",
        "\n",
        "    # Display chat history\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.markdown(message[\"content\"])\n",
        "            if \"sources\" in message and message[\"sources\"]:\n",
        "                with st.expander(\"üìö View Sources\"):\n",
        "                    for i, source in enumerate(message[\"sources\"], 1):\n",
        "                        st.markdown(f\"**Source {i}:**\")\n",
        "                        st.markdown(f\"```{source['text']}```\")\n",
        "\n",
        "    # Chat input\n",
        "    if prompt := st.chat_input(\"Ask a question about your document\"):\n",
        "        # Add user message\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(prompt)\n",
        "\n",
        "        # Get AI response\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"ü§î Thinking...\"):\n",
        "                # Here you'd call your RAG system\n",
        "                # result = st.session_state.rag_system.query(prompt)\n",
        "\n",
        "                # Placeholder response\n",
        "                response = \"This is where the AI response would appear!\"\n",
        "                sources = []\n",
        "\n",
        "                st.markdown(response)\n",
        "\n",
        "                # Add assistant message\n",
        "                st.session_state.messages.append({\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": response,\n",
        "                    \"sources\": sources\n",
        "                })\n",
        "\n",
        "else:\n",
        "    st.info(\"üëÜ Please upload a PDF and enter your API key to start chatting!\")\n",
        "\n",
        "    # Instructions\n",
        "    with st.expander(\"üìñ How to use\"):\n",
        "        st.markdown(\"\"\"\n",
        "        1. **Get API Key**: Visit [Google AI Studio](https://makersuite.google.com/app/apikey)\n",
        "        2. **Upload PDF**: Choose any PDF document you want to chat with\n",
        "        3. **Start Chatting**: Ask questions about the document content\n",
        "        4. **View Sources**: Expand source sections to see where answers come from\n",
        "        \"\"\")\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"Built with ‚ù§Ô∏è using LlamaIndex, Gemini, and Streamlit\")\n",
        "'''\n",
        "\n",
        "    # Save Streamlit code to file\n",
        "    with open('streamlit_app.py', 'w') as f:\n",
        "        f.write(streamlit_code)\n",
        "\n",
        "    print(\"üì± Streamlit app code saved to 'streamlit_app.py'\")\n",
        "    return streamlit_code\n",
        "\n",
        "# Create the Streamlit app\n",
        "streamlit_code = create_streamlit_app()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7ioPDbhw1Z_",
        "outputId": "3439e9c2-cb67-464a-b238-e0055588cf7d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì± Streamlit app code saved to 'streamlit_app.py'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_streamlit_with_ngrok():\n",
        "    \"\"\"\n",
        "    Set up Streamlit with ngrok for public access\n",
        "    \"\"\"\n",
        "    print(\"üåê Setting up public web interface...\")\n",
        "\n",
        "    # Install pyngrok if not already installed\n",
        "    try:\n",
        "        import pyngrok\n",
        "    except ImportError:\n",
        "        print(\"Installing pyngrok...\")\n",
        "        !pip install -q pyngrok\n",
        "        import pyngrok\n",
        "\n",
        "    from pyngrok import ngrok\n",
        "    import threading\n",
        "    import time\n",
        "\n",
        "    # Kill any existing ngrok tunnels\n",
        "    ngrok.kill()\n",
        "\n",
        "    # Function to run Streamlit\n",
        "    def run_streamlit():\n",
        "        !streamlit run streamlit_app.py --server.port 8501 --server.headless true --server.fileWatcherType none --browser.gatherUsageStats false\n",
        "\n",
        "    # Start Streamlit in background\n",
        "    threading.Thread(target=run_streamlit, daemon=True).start()\n",
        "\n",
        "    # Wait a bit for Streamlit to start\n",
        "    time.sleep(10)\n",
        "\n",
        "    # Create ngrok tunnel\n",
        "    try:\n",
        "        public_url = ngrok.connect(8501, proto=\"http\", bind_tls=True)\n",
        "        print(f\"\\nüéâ SUCCESS! Your chatbot is now live at:\")\n",
        "        print(f\"üîó {public_url}\")\n",
        "        print(f\"\\nüì± Open this URL in any browser to use your chatbot!\")\n",
        "        print(f\"üí° This URL will work from any device with internet access\")\n",
        "\n",
        "        return public_url\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error setting up ngrok: {e}\")\n",
        "        print(\"üí° Try running these commands manually in separate cells:\")\n",
        "        print(\"   1. !streamlit run streamlit_app.py &\")\n",
        "        print(\"   2. from pyngrok import ngrok; print(ngrok.connect(8501))\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "E_HyHEN-xIjw"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}